{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/prateekchhikara/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import google.generativeai as genai\n",
    "import editdistance\n",
    "\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from mistralai import Mistral\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import warnings\n",
    "import google.generativeai as genai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentancePicker(flag,dataSet):\n",
    "    if flag==0:\n",
    "        val=20\n",
    "    elif flag==1:\n",
    "        val=50\n",
    "    else:\n",
    "        val=100\n",
    "\n",
    "    tempSet=set()\n",
    "    n=len(dataSet)\n",
    "\n",
    "    while(len(tempSet)<val):\n",
    "        idx=np.random.randint(0,n)\n",
    "        tempSet.add(dataSet[idx][0])\n",
    "    \n",
    "    return list(tempSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocessing():\n",
    "    data=pd.read_csv('dsbanglore.csv', sep='delimiter', header=None ,engine='python')\n",
    "    data=np.array(data)\n",
    "    n=data.shape[0]\n",
    "    list=[]\n",
    "    for i in range(n):\n",
    "        list.append(data[i]) \n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(list_original, item) :\n",
    "    text = word_tokenize(item)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    # print(tagged)\n",
    "    imp = set()\n",
    "    for each in tagged :\n",
    "        if(each[1] == 'JJ' or each[1] == 'JJR' or each[1] == 'JJS' or each[1] == 'NN' or each[1] == 'NNS' or each[1] == 'NNP' or each[1] == 'NNPS' or each[1] == 'VB' or each[1] == 'VBD' or each[1] == 'VBG' or each[1] == 'VBN' or each[1] == 'VBP' or each[1] == 'VBZ') :\n",
    "            imp.add(each[0])\n",
    "    \n",
    "    max = 0 \n",
    "    final = \"\"\n",
    "    for each_org in list_original :\n",
    "        count = 0 \n",
    "        for each in imp :\n",
    "            if each in each_org :\n",
    "                count += 1\n",
    "        if(count > max) :\n",
    "            max = count \n",
    "            final = each_org\n",
    "    \n",
    "    return final \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(list_original, list_llm) :\n",
    "\n",
    "    error = 30\n",
    "    list_ans = set()\n",
    "\n",
    "    for item in list_llm :\n",
    "        if(len(item) > 4 and (item[1] == '.' or item[2] == \".\" or item[3] == \".\" or item[4] == \".\" or item[1] == ':' or item[2] == \":\" or item[3] == \":\" or item[4] == \":\")) :\n",
    "            updated_item = item[item.index('.')+1:].strip()\n",
    "            # print(updated_item)\n",
    "\n",
    "            if(updated_item not in list_original) :\n",
    "\n",
    "                edit_distance = 10000\n",
    "                edit_string = \"\"\n",
    "                for org_item in list_original :\n",
    "                    # print(org_item)\n",
    "                    present_dist = editdistance.eval(updated_item, org_item)\n",
    "\n",
    "                    if present_dist < edit_distance :\n",
    "                        edit_distance = present_dist\n",
    "                        edit_string = org_item\n",
    "\n",
    "                if(edit_distance < error) :\n",
    "                    list_ans.add(edit_string) \n",
    "                    print(updated_item , \"------>\", edit_string, \"------->\", edit_distance) #, len(updated_item))\n",
    "                else :\n",
    "                    similar = find(list_original, updated_item)\n",
    "                    if(len(similar) > 0) :\n",
    "                      list_ans.add(similar)\n",
    "                      print(updated_item , \"------>\", similar)\n",
    "\n",
    "            else :\n",
    "                list_ans.add(updated_item) \n",
    "\n",
    "    return list_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_claude(model_name, num_tokens, sp, up):\n",
    "\n",
    "    claude_client = anthropic.Anthropic(api_key = \"\")\n",
    "    while True : \n",
    "        try :\n",
    "            message = claude_client.messages.create(\n",
    "                    model=model_name,\n",
    "                    max_tokens=num_tokens,\n",
    "                    temperature=0,\n",
    "                    top_p = 1,\n",
    "                    system= sp,\n",
    "                    messages=[{\"role\": \"user\", \"content\": up}]\n",
    "            )\n",
    "            break \n",
    "\n",
    "        except Exception as e:\n",
    "            print(model_name + \" went into exception ....\", e)\n",
    "            print(\"Sleeping for 2 minutes\")\n",
    "            time.sleep(120) \n",
    "\n",
    "    \n",
    "    return message.content[0].text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_model(num_tokens , sp , up):\n",
    "\n",
    "    \n",
    "    client = Groq(api_key = \"\")\n",
    "    while True : \n",
    "        try :\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "                messages=[{\"role\": \"system\", \"content\": sp},{\"role\": \"user\", \"content\": up}],\n",
    "                temperature=0,\n",
    "                max_completion_tokens=num_tokens,\n",
    "                top_p=1,\n",
    "                stream=False,\n",
    "                stop=None,\n",
    "            )\n",
    "            break \n",
    "        except Exception as e:\n",
    "            print(\"llama went into exception ....\", e)\n",
    "            print(\"Sleeping for 2 minutes\")\n",
    "            time.sleep(70) \n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "    return completion.choices[0].message.content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mixtral(num_tokens, sp, up) :\n",
    "    mixtral_client = Mistral(api_key=\"\")\n",
    "    while True : \n",
    "        try :\n",
    "\n",
    "            completion = mixtral_client.chat.complete(\n",
    "                        model = \"open-mistral-nemo\",\n",
    "                        messages=[{\"role\": \"system\", \"content\": sp},{\"role\": \"user\", \"content\": up}],\n",
    "                        temperature=0,\n",
    "                        max_tokens=num_tokens,\n",
    "                        top_p=1.0,\n",
    "            )\n",
    "\n",
    "            break \n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"open-mistral-nemo went into exception ....\", e)\n",
    "            print(\"Sleeping for 2 minutes\")\n",
    "            time.sleep(70) \n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "    return completion.choices[0].message.content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemmini(list , max_token , system_prompt, user_prompt):\n",
    "    genai.configure(api_key=\"\")\n",
    "\n",
    "    res_str = \"\"\n",
    "    for i,each_sent in enumerate(list) :\n",
    "        res_str += str(i+1) + \". \" + each_sent + \"\\n\" ;\n",
    "\n",
    "\n",
    "    generation_config = {\n",
    "      \"temperature\": 0,\n",
    "      \"top_p\": 1,\n",
    "      \"top_k\": 1,\n",
    "      \"max_output_tokens\": max_token,\n",
    "      \"response_mime_type\": \"text/plain\",\n",
    "    }\n",
    "   \n",
    "    model = genai.GenerativeModel(\n",
    "      model_name=\"gemini-2.0-flash-lite-preview-02-05\",\n",
    "      generation_config=generation_config,\n",
    "      system_instruction=system_prompt,\n",
    "    )\n",
    "    \n",
    "    chat_session = model.start_chat(\n",
    "      history=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"parts\": [user_prompt],\n",
    "       \n",
    "        },\n",
    "      ]\n",
    "    ) \n",
    "    \n",
    "    response = chat_session.send_message(res_str)\n",
    "    response=response.text\n",
    "    ans=response.strip().split(\"\\n\")\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDict={}\n",
    "def main():\n",
    "    \n",
    "    processedData=preprocessing() \n",
    "\n",
    "    for i in range(9):\n",
    "        size=0\n",
    "        selected_elements=0\n",
    "        list=[]\n",
    "        if (i<=2):\n",
    "            list=sentancePicker(0,processedData)\n",
    "            selected_elements=10            \n",
    "            size=20\n",
    "        elif ( i>=3 and i<=5):\n",
    "            list=sentancePicker(1,processedData)\n",
    "            selected_elements=25\n",
    "            size=50\n",
    "        else :\n",
    "            list=sentancePicker(2, processedData)\n",
    "            selected_elements=50\n",
    "            size=100\n",
    "            \n",
    "        dict={}\n",
    "        for j in range(len(list)):\n",
    "            tempList=  [0,0,0,0,0,0,0,0,0,0,0]\n",
    "            dict[list[j]]=tempList\n",
    "            \n",
    "            \n",
    "        res = \"\"\n",
    "        for j,each_sent in enumerate(list) :\n",
    "            res += str(j+1) + \". \" + each_sent + \"\\n\" ;\n",
    "\n",
    "        for j in range(10):\n",
    "\n",
    "            system_instruction= \"Select best \"+ str(selected_elements)+ \"sentences that summarizes the input text. Output the sentence along with the sentence number that best summarizes the above input. Think step by step and follow the instructions.\"\n",
    "            user_instruction  = \"Input consist of \" + str(size) + \" sentences. Each sentence is present in a new line. Each sentence contains a sentence number followed by text. You are an assistant that selects best \"+ str(selected_elements)+ \" sentences (subset) which summarizes the input. Note that - summary should not contain duplicates and there should be no paraphrasing.\\n\\n\"\n",
    "            outputList=[]\n",
    "           \n",
    "            if(i%3==0):\n",
    "                outputList=gemmini(list ,8000,system_instruction , user_instruction)\n",
    "            elif(i%3==1):\n",
    "                outputList=model_mixtral(8000,system_instruction , user_instruction+res)\n",
    "            else:\n",
    "                outputList=llama_model(8000,system_instruction,user_instruction+res)\n",
    "\n",
    "     \n",
    "            finalList=editDistance(list , outputList)\n",
    "\n",
    "            for k in finalList:\n",
    "                    dict[k][j]+=1\n",
    "            \n",
    "        modelName=\"\"\n",
    "        # total probability ke leye\n",
    "        if(i%3==0):\n",
    "            modelName=\"gemini\"\n",
    "        elif(i%3==1):\n",
    "            modelName=\"mixtral\"\n",
    "        else:\n",
    "            modelName=\"llama\"\n",
    "\n",
    "        for each in list:\n",
    "            dict[each][10]=0\n",
    "            for k in range(10):\n",
    "                dict[each][10]=dict[each][10]+dict[each][k]\n",
    "        # print(i)\n",
    "        # print(modelName)\n",
    "        keyList=[]\n",
    "        valList=[]\n",
    "        for vals in dict:\n",
    "            keyList.append(vals)\n",
    "            valList.append(dict.get(vals))\n",
    "        keyList=np.array(keyList)\n",
    "        keyList=keyList.reshape(-1,1)\n",
    "        valList=np.array(valList)\n",
    "        \n",
    "        fans=np.hstack((keyList,valList))\n",
    "        fans=pd.DataFrame(fans)\n",
    "        fans.to_excel(modelName+'_'+str(size)+\"\"+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
